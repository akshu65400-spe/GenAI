# -*- coding: utf-8 -*-
"""Ai1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xm9lweha0nt8nFD99CCTfHwF2Ry7W_jp
"""

from langchain.prompts import FewShotPromptTemplate, PromptTemplate
from langchain.chains import LLMChain
from langchain_openai import AzureChatOpenAI
from dotenv import load_dotenv
import os

load_dotenv()

llm = AzureChatOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    deployment_name=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT"),
    model_name = "gpt-5-mini",
    temperature=0.7
)

examples = [
    {"question": "What is the capital of India", "answer": "The captial of India is"},
    {"question": "What is the capital of Spain", "answer": "The captial of Spain is"},
    {"question": "What is the capital of China", "answer": "The captial of China is"}
]

example_prompt = PromptTemplate(
    template = "question: {question} \n Answer:{answer}"
)

few_short_tem = FewShotPromptTemplate(
    example = examples,
    input_variables = ['question'],
    suffix = "Answer: ",
    prefix = "You are a helpful assistant Answer the following questions"
)

pipeline = LLMChain(
    llm=llm,
    prompt=few_short_tem
)

pipeline.invoke(question="What is the captial of Japan")